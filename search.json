[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this blog"
  },
  {
    "objectID": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html",
    "href": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html",
    "title": "Post With Code",
    "section": "",
    "text": "A top-down approach on how to ‘Marie Kondo’ your data sets into Hadley Wickham’s definition of tidy data by introducing pandas and using it’s functionality to introduce Python.\nMarie Kondo famously asks us to clean our homes/spaces, discarding things that do not bring us joy and treasuring things that do. Hadley Wickham’s papers shows us how to translate that philosophy into messy datasets, and create ones that instead of bringing panic, bring joy. Tidy data has become the standard format for science and business because it allows people to easily turn a data table into graphs, analysis and insight.\nThe purpose of this repository is to help users coming from Excel, to create tidy data sets using Pandas by working through examples that deal with various messy datasets used in Wickham’s paper. Through the process of reading messy data, processing into tidy data and then performing basic analysis, it will also help with general better practice for data management.\nThe big advantage of Tidy Data is that it makes a clear distinction between a variable, an observation and a value. In this way, all data is standardised which makes it easier to collaborate on. Whether thinking about collaborators as your current colleagues, your future self, or future peers, organising and sharing data in a consistent and predictable way means less adjustment, time, and effort for all."
  },
  {
    "objectID": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#the-pros-and-cons-of-tidy-data",
    "href": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#the-pros-and-cons-of-tidy-data",
    "title": "Post With Code",
    "section": "The Pros and Cons of Tidy Data",
    "text": "The Pros and Cons of Tidy Data\nIf you’ve read this far and are sitting there questioning why you would want a long table instead of a wide excel table where all column headers can be seen, well, the short answer is that not all data needs to be tidy. As Marie Kondo says, “If it sparks joy, keep it with confidence”. Both tidy and messy data are useful. Choose the format that makes analysis easier. Tidy data is simply another tool to keep sharp, so that when it is needed, you can put it to good use.\n\n\n\n\n\n\n\nAdvantages of Tidy Data\nAdvantages of Messy Data\n\n\n\n\nBecause tidy data is a standard way of structuring a dataset, it is easy for an analyst or a computer to extract needed variables.\nEfficient storage for completely crossed designs which can lead to efficient computation if desired operations can be expressed as matric operations.\n\n\nMany analysis operations, including all aggregation functions, involve all of the values in a variable.\nPresentation\n\n\n\nEasier data capturing\n\n\n\n\nWhat is Tidy Data?\nAccording to Wickham, in a tidy data set, every column is a variable, every row is an observation and every cell is a single value.\n\nMessy datasets, by extension, violate these 3 rules in some way. The five most common problems are:\n\nColumn headers are values, not variable names.\nMultiple variables are stored in one column.\n\nVariables are stored in both rows and columns.\nMultiple types of observational units are stored in the same table.\nA single observational unit is stored in multiple tables.\n\nEssentially, we want to take a wide dataset and transform it into a long one. Consider the following table.\n\n\n\nName\nTreatment A\nTreatment B\n\n\n\n\nJohn Smith\n-\n2\n\n\nJane Doe\n16\n11\n\n\nMary Johnson\n3\n1\n\n\n\nThe information has been presented in a common way, however, it could be reorganised making the value, variables and observation more clear, as seen below.\n\n\n\nName\nTreatment\nValue\n\n\n\n\nJohn Smith\nA\n-\n\n\nJohn Smith\nB\n2\n\n\nJane Doe\nA\n16\n\n\nJane Doe\nB\n11\n\n\nMary Johnson\nA\n3\n\n\nMary Johnson\nB\n1"
  },
  {
    "objectID": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#cleaning-tools",
    "href": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#cleaning-tools",
    "title": "Post With Code",
    "section": "‘Cleaning’ tools",
    "text": "‘Cleaning’ tools\nThe ‘cleaning’ tools we use to deal with these violations are: - Melting/Unpivoting (changing columns into rows) - Casting/Unstack (changing rows to columns) - String manipulation - Splitting - Regular expressions\n\nMelting\npandas.melt() unpivots a DataFrame from wide format to long format.\n\n# importing pandas as pd\nimport pandas as pd\n  \n# creating a dataframe\ndf = pd.DataFrame({'Name': {0: 'Jennifer', 1: 'Susan', 2: 'Tami'},\n                   'Course': {0: 'Honours', 1: 'Doctorate', 2: 'Graduate'},\n                   'Age': {0: 35, 1: 41, 2: 32}})\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Course\n      Age\n    \n  \n  \n    \n      0\n      Jennifer\n      Honours\n      35\n    \n    \n      1\n      Susan\n      Doctorate\n      41\n    \n    \n      2\n      Tami\n      Graduate\n      32\n    \n  \n\n\n\n\n\n# Name is id_vars and Course is value_vars\npd.melt(df, id_vars =['Name'], value_vars =['Course'])\n\n\n\n\n\n  \n    \n      \n      Name\n      variable\n      value\n    \n  \n  \n    \n      0\n      Jennifer\n      Course\n      Honours\n    \n    \n      1\n      Susan\n      Course\n      Doctorate\n    \n    \n      2\n      Tami\n      Course\n      Graduate\n    \n  \n\n\n\n\n\n\nUnstacking\npandas.unstack() is a function that pivots the level of the indexed columns in a stacked dataframe. A stacked dataframe is usually a result of an aggregated groupby function in pandas. stack() sets the columns to a new level of hierarchy whereas unstack() pivots the indexed column.\n\n# import the python pandas package\nimport pandas as pd\n \n# create a sample dataframe\ndf = pd.DataFrame({\"cars\": [\"VW\", \"VW\", \"Renault\", \"Renault\"],\n                     \"sales_jan\": [20, 22, 24, 26],\n                     \"sales_feb\": [11, 13, 15, 17]},\n                    columns=[\"cars\", \"sales_jan\",\n                             'sales_feb'])\ndf\n\n# aggregate the car sales data by sum min\n# and max sales of two quarters as shown\ngrouped_df = df.groupby('cars').agg(\n    {\"sales_jan\": [sum, max],\n     \"sales_feb\": [sum, min]})\n\ngrouped_df\n\n\n\n\n\n  \n    \n      \n      sales_jan\n      sales_feb\n    \n    \n      \n      sum\n      max\n      sum\n      min\n    \n    \n      cars\n      \n      \n      \n      \n    \n  \n  \n    \n      Renault\n      50\n      26\n      32\n      15\n    \n    \n      VW\n      42\n      22\n      24\n      11\n    \n  \n\n\n\n\n\n# Unstacking the grouped dataframe\ngrouped_df.unstack()\n\n                cars   \nsales_jan  sum  Renault    50\n                VW         42\n           max  Renault    26\n                VW         22\nsales_feb  sum  Renault    32\n                VW         24\n           min  Renault    15\n                VW         11\ndtype: int64\n\n\n\n\nString Manipulation\nString manipulation is the process of changing, parsing, splicing, pasting, or analyzing strings. Sometimes, data in the string is not suitable for manipulating the analysis or get a description of the data.\nThere are many applications of string manipulation, below is a quick example of how to change the data type to string.\n\n# Importing the necessary libraries\nimport pandas as pd\nimport numpy as np\n  \n# Creating a df\ndf = pd.Series(['Amanda', 'Bob', 'Claire', 'Devon', 'Evan', np.nan, 'Fred'])\ndf\n\n# %%\n# We can change the dtype after creation of dataframe\ndf.astype('string')\n\n0    Amanda\n1       Bob\n2    Claire\n3     Devon\n4      Evan\n5      <NA>\n6      Fred\ndtype: string\n\n\n\n\nSplitting\nSplitting is as the name suggests, it gives us power to split a data frame depending on the desired outcome.\nThere are many applications of splitting (splitting by rows, splitting a df into smaller df’s…), below is a quick example of splitting a text column into 2 columns.\n\n# import Pandas as pd\nimport pandas as pd\n\n# create a new data frame\ndf = pd.DataFrame({'Name': ['Peter Parker', 'Bruce Wayne', 'Clark Kent'],\n                'Age':[32, 34, 36]})\n\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Age\n    \n  \n  \n    \n      0\n      Peter Parker\n      32\n    \n    \n      1\n      Bruce Wayne\n      34\n    \n    \n      2\n      Clark Kent\n      36\n    \n  \n\n\n\n\n\n# Adding 2 new columns by splitting up Name\n# by default splitting is done on the basis of single space.\n# Naming new columns First and Last\ndf[['First','Last']] = df.Name.str.split(expand=True)\ndf\n\n\n\n\n\n  \n    \n      \n      Name\n      Age\n      First\n      Last\n    \n  \n  \n    \n      0\n      Peter Parker\n      32\n      Peter\n      Parker\n    \n    \n      1\n      Bruce Wayne\n      34\n      Bruce\n      Wayne\n    \n    \n      2\n      Clark Kent\n      36\n      Clark\n      Kent\n    \n  \n\n\n\n\n\ndf.Name.apply(lambda x: pd.Series(str(x).split()))\n\n\n\n\n\n  \n    \n      \n      0\n      1\n    \n  \n  \n    \n      0\n      Peter\n      Parker\n    \n    \n      1\n      Bruce\n      Wayne\n    \n    \n      2\n      Clark\n      Kent\n    \n  \n\n\n\n\n\n\nRegular Expressions\nRegular expressions (regex) are essentially text patterns that you can use to automate searching through and replacing elements within strings of text. This can make cleaning and working with text-based data sets much easier, saving you the trouble of having to search through mountains of text by hand.\n\ndf = pd.DataFrame(\n    {\n        'City':['New York (City)', 'Parague', 'New Delhi (Delhi)', 'Venice', 'new Orleans'],\n        'Event':['Music', 'Poetry', 'Theatre', 'Comedy', 'Tech_Summit'],\n        'Cost':[10000, 5000, 15000, 2000, 12000]\n    }\n)\n  \ndf\n\n\n\n\n\n  \n    \n      \n      City\n      Event\n      Cost\n    \n  \n  \n    \n      0\n      New York (City)\n      Music\n      10000\n    \n    \n      1\n      Parague\n      Poetry\n      5000\n    \n    \n      2\n      New Delhi (Delhi)\n      Theatre\n      15000\n    \n    \n      3\n      Venice\n      Comedy\n      2000\n    \n    \n      4\n      new Orleans\n      Tech_Summit\n      12000\n    \n  \n\n\n\n\n\n# Importing re package for using regular expressions\nimport re\n\n# Function to clean the names\ndef Clean_names(City_name):\n    # Search for opening bracket in the name followed by\n    # any characters repeated any number of times\n    if re.search('\\(.*', City_name):\n\n        # Extract the position of beginning of pattern\n        pos = re.search('\\(.*', City_name).start()\n\n        # return the cleaned name\n        return City_name[:pos]\n\n    else:\n        # if clean up needed return the same name\n        return City_name\n        \n# Updated the city columns\ndf['City'] = df['City'].apply(Clean_names)\n\n# Print the updated dataframe\ndf\n\n\n\n\n\n  \n    \n      \n      City\n      Event\n      Cost\n    \n  \n  \n    \n      0\n      New York\n      Music\n      10000\n    \n    \n      1\n      Parague\n      Poetry\n      5000\n    \n    \n      2\n      New Delhi\n      Theatre\n      15000\n    \n    \n      3\n      Venice\n      Comedy\n      2000\n    \n    \n      4\n      new Orleans\n      Tech_Summit\n      12000"
  },
  {
    "objectID": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#where-to-go-from-here",
    "href": "posts/tidy-data-with-python/2022-03-07-tidy-data-with-python.html#where-to-go-from-here",
    "title": "Post With Code",
    "section": "Where to go from here",
    "text": "Where to go from here\n\nDownload the repo\nRead through Hadley Wickham’s paper on Tidy data. While this post has summarized some of the paper, going through the examples will help you hugely when working through the exercises.\nGo through ‘README’ carefully and use the extra resources on pandas as needed.\nExercises 1 through to 4b, gradually provide you with less and less scaffolding, work in each notebook to try and create tidy data sets. To check your answers, use the solutions file.\nHave fun bringing joy to your data sets!"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DataQ Blog",
    "section": "",
    "text": "Post With Code\n\n\nA top-down approach on how to ‘Marie Kondo’ your data sets\n\n\n\n\npython\n\n\ndata analysis\n\n\n\n\n\n\n\n\n\n\n\nJan 6, 2023\n\n\nJen Godlonton\n\n\n\n\n\n\nNo matching items"
  }
]